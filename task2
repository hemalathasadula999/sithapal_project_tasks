Install the following library to run the following code:-pip install requests beautifulsoup4 sentence-transformers faiss-cpu openai
import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np

# Step 1: Crawl and Scrape Website Content
def scrape_website(url):
    """
    Crawls and scrapes text content from the given URL.
    """
    response = requests.get(url)
    if response.status_code != 200:
        print(f"Failed to fetch {url}. Status code: {response.status_code}")
        return []

    soup = BeautifulSoup(response.content, "html.parser")
    
    # Extract visible text
    paragraphs = soup.find_all("p")
    text_chunks = [para.get_text(strip=True) for para in paragraphs if para.get_text(strip=True)]
    return text_chunks

# Step 2: Generate Vector Embeddings
def generate_embeddings(text_chunks, model_name="sentence-transformers/all-MiniLM-L6-v2"):
    """
    Converts text chunks into vector embeddings using a pre-trained model.
    """
    model = SentenceTransformer(model_name)
    embeddings = model.encode(text_chunks, convert_to_tensor=True)
    return embeddings, model

# Step 3: Store Embeddings in a Vector Database
def store_embeddings(embeddings):
    """
    Stores embeddings in a FAISS vector database for similarity search.
    """
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings.cpu().numpy())  # FAISS expects NumPy arrays
    return index

# Step 4: Handle Query and Retrieve Relevant Data
def query_vector_database(query, model, index, text_chunks, top_k=3):
    """
    Handles user queries by finding the most relevant text chunks.
    """
    query_embedding = model.encode([query], convert_to_tensor=True).cpu().numpy()
    distances, indices = index.search(query_embedding, top_k)
    results = [text_chunks[i] for i in indices[0]]
    return results

# Step 5: Generate a Response Using GPT
def generate_response(retrieved_text, query, openai_api_key):
    """
    Generates a response using GPT by integrating retrieved text.
    """
    import openai
    openai.api_key = openai_api_key

    context = "\n".join(retrieved_text)
    prompt = f"Context: {context}\n\nQuestion: {query}\n\nAnswer:"
    
    response = openai.Completion.create(
        engine="gpt-4",
        prompt=prompt,
        max_tokens=200,
        temperature=0
    )
    return response["choices"][0]["text"].strip()

# Full Workflow Example
def main():
    urls = [
        "https://www.uchicago.edu/",
        "https://www.washington.edu/",
        "https://www.stanford.edu/",
        "https://und.edu/"
    ]
    query = "What are the main academic programs offered at these universities?"  # User query
    openai_api_key = "your_openai_api_key_here"

    # Step 1: Scrape data from websites
    all_text_chunks = []
    for url in urls:
        text_chunks = scrape_website(url)
        all_text_chunks.extend(text_chunks)

    # Step 2: Generate embeddings
    embeddings, model = generate_embeddings(all_text_chunks)

    # Step 3: Store embeddings in a vector database
    index = store_embeddings(embeddings)

    # Step 4: Query handling
    retrieved_text = query_vector_database(query, model, index, all_text_chunks)

    # Step 5: Generate response
    response = generate_response(retrieved_text, query, openai_api_key)

    print("Query Response:\n", response)

if _name_ == "_main_":
    main()
